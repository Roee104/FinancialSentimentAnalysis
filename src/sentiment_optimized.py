# src/sentiment_optimized.py

"""
Optimized sentiment analysis using the best parameters found through testing.
This version properly reduces neutral bias while maintaining accuracy.
"""

import torch
import torch.nn.functional as F
from typing import List, Dict
from collections import defaultdict

from sentiment import FinBERTSentimentAnalyzer


class OptimizedSentimentAnalyzer(FinBERTSentimentAnalyzer):
    """
    Optimized FinBERT analyzer with proven bias reduction parameters
    """

    def __init__(self,
                 model_name: str = "yiyanghkust/finbert-tone",
                 device: str = None):
        super().__init__(model_name, device)

        # Optimized parameters from testing
        self.neutral_penalty = 0.5    # Strong neutral reduction
        self.pos_boost = 1.1          # Modest positive boost
        self.neg_boost = 1.1          # Modest negative boost
        self.min_confidence_diff = 0.03  # Lower threshold for classification

        # Statistics tracking
        self.stats = defaultdict(int)

    def _apply_bias_correction(self, probs: torch.Tensor) -> torch.Tensor:
        """
        Apply optimized bias correction to reduce neutral overconfidence
        """
        corrected_probs = probs.clone()

        # Apply corrections
        corrected_probs[:, 0] *= self.neg_boost    # Negative class
        corrected_probs[:, 1] *= self.neutral_penalty  # Neutral class (reduce)
        corrected_probs[:, 2] *= self.pos_boost    # Positive class

        # Renormalize to ensure probabilities sum to 1
        corrected_probs = F.softmax(corrected_probs, dim=1)

        return corrected_probs

    def predict_optimized(self,
                          texts: List[str],
                          batch_size: int = 16,
                          max_length: int = 512) -> List[Dict]:
        """
        Predict sentiment with optimized bias correction
        """
        if not texts:
            return []

        results = []

        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            enc = self.tokenizer(batch,
                                 padding=True,
                                 truncation=True,
                                 max_length=max_length,
                                 return_tensors="pt")
            input_ids = enc["input_ids"].to(self.device)
            attention_mask = enc["attention_mask"].to(self.device)

            with torch.no_grad():
                outputs = self.model(input_ids=input_ids,
                                     attention_mask=attention_mask)

                # Get original probabilities
                original_probs = F.softmax(outputs.logits, dim=-1)

                # Apply bias correction
                corrected_probs = self._apply_bias_correction(original_probs)

                # Classify with improved logic
                for idx, text in enumerate(batch):
                    prob_neg = corrected_probs[idx][0].item()
                    prob_neu = corrected_probs[idx][1].item()
                    prob_pos = corrected_probs[idx][2].item()

                    # Get original prediction for comparison
                    orig_max_idx = torch.argmax(original_probs[idx]).item()
                    original_label = ["Negative",
                                      "Neutral", "Positive"][orig_max_idx]

                    # Determine final classification
                    if prob_pos > prob_neg and prob_pos > prob_neu:
                        # Check if positive signal is strong enough
                        if prob_pos - max(prob_neg, prob_neu) >= self.min_confidence_diff:
                            label = "Positive"
                            confidence = prob_pos
                            self.stats['positive_classified'] += 1
                        else:
                            label = "Neutral"
                            confidence = prob_neu
                            self.stats['neutral_classified'] += 1
                    elif prob_neg > prob_pos and prob_neg > prob_neu:
                        # Check if negative signal is strong enough
                        if prob_neg - max(prob_pos, prob_neu) >= self.min_confidence_diff:
                            label = "Negative"
                            confidence = prob_neg
                            self.stats['negative_classified'] += 1
                        else:
                            label = "Neutral"
                            confidence = prob_neu
                            self.stats['neutral_classified'] += 1
                    else:
                        label = "Neutral"
                        confidence = prob_neu
                        self.stats['neutral_classified'] += 1

                    # Track changes
                    if original_label != label:
                        self.stats[f'changed_{original_label.lower()}_to_{label.lower()}'] += 1

                    score_dict = {
                        "Negative": prob_neg,
                        "Neutral": prob_neu,
                        "Positive": prob_pos
                    }

                    results.append({
                        "text": text,
                        "label": label,
                        "confidence": confidence,
                        "scores": score_dict,
                        "original_label": original_label,
                        "bias_corrected": original_label != label
                    })

        return results

    def get_stats(self) -> Dict[str, int]:
        """Get processing statistics"""
        return dict(self.stats)

    def reset_stats(self):
        """Reset statistics"""
        self.stats.clear()


def compare_analyzers(texts: List[str]) -> Dict:
    """
    Compare original vs optimized sentiment analysis
    """
    print("ğŸ” Comparing Original vs Optimized Sentiment Analysis")
    print("=" * 55)

    # Original FinBERT
    print("Running original FinBERT...")
    original = FinBERTSentimentAnalyzer()
    original_results = original.predict(texts)

    # Optimized FinBERT
    print("Running optimized FinBERT...")
    optimized = OptimizedSentimentAnalyzer()
    optimized_results = optimized.predict_optimized(texts)

    # Calculate distributions
    original_dist = _get_distribution(original_results)
    optimized_dist = _get_distribution(optimized_results)

    # Calculate improvements
    neutral_change = original_dist.get(
        'Neutral', 0) - optimized_dist.get('Neutral', 0)

    print("\nğŸ“Š RESULTS COMPARISON")
    print("=" * 30)

    print("Original FinBERT:")
    for label, pct in original_dist.items():
        print(f"  {label}: {pct:.1f}%")

    print("\nOptimized FinBERT:")
    for label, pct in optimized_dist.items():
        print(f"  {label}: {pct:.1f}%")

    print(
        f"\nğŸ¯ Neutral Bias Reduction: {neutral_change:.1f} percentage points")

    if neutral_change > 0:
        print("âœ… SUCCESS: Neutral bias reduced!")
    else:
        print("âŒ Issue: Neutral bias not reduced")

    # Show example changes
    print("\nğŸ“ Example Changes:")
    changes_shown = 0
    for orig, opt in zip(original_results, optimized_results):
        if orig['label'] != opt['label'] and changes_shown < 5:
            text_preview = orig['text'][:50] + \
                "..." if len(orig['text']) > 50 else orig['text']
            print(f"\n{changes_shown + 1}. {text_preview}")
            print(f"   Original: {orig['label']} ({orig['confidence']:.3f})")
            print(f"   Optimized: {opt['label']} ({opt['confidence']:.3f})")
            changes_shown += 1

    # Show optimization statistics
    stats = optimized.get_stats()
    if stats:
        print(f"\nğŸ“ˆ Optimization Statistics:")
        for stat, count in stats.items():
            print(f"  {stat}: {count}")

    return {
        "original_distribution": original_dist,
        "optimized_distribution": optimized_dist,
        "neutral_reduction": neutral_change,
        "optimization_stats": stats
    }


def _get_distribution(results: List[Dict]) -> Dict[str, float]:
    """Get percentage distribution of sentiment labels"""
    if not results:
        return {}

    counts = defaultdict(int)
    for result in results:
        counts[result["label"]] += 1

    total = len(results)
    return {label: count/total*100 for label, count in counts.items()}


def test_on_real_data(n_samples: int = 500) -> Dict:
    """Test the optimized analyzer on real data"""

    print(f"\nğŸ§ª Testing on Real Data ({n_samples} samples)")
    print("=" * 45)

    try:
        import pandas as pd
        df = pd.read_parquet("data/financial_news_2020_2025_100k.parquet")

        # Sample articles
        sample_df = df.sample(n=min(n_samples, len(df)), random_state=42)

        # Prepare texts (shorter for efficiency)
        texts = []
        for _, row in sample_df.iterrows():
            title = str(row['title']) if pd.notna(row['title']) else ""
            content = str(row['content']) if pd.notna(row['content']) else ""
            content_short = " ".join(content.split()[:75])  # First 75 words
            full_text = f"{title}. {content_short}"
            texts.append(full_text)

        print(f"âœ… Loaded {len(texts)} articles for testing")

    except Exception as e:
        print(f"âŒ Could not load real data: {e}")
        print("Using predefined test texts instead...")

        texts = [
            "Apple reported strong quarterly earnings that exceeded analyst expectations by 10%.",
            "The company faced significant supply chain challenges affecting profit margins.",
            "Stock price remained relatively stable throughout the trading session today.",
            "Investors expressed optimism about the company's growth prospects and expansion plans.",
            "Regulatory concerns continue to weigh on market sentiment for the sector.",
            "The quarterly dividend payment was maintained at current levels as expected.",
            "Revenue growth was modest compared to the previous quarter's performance.",
            "Management provided updated guidance that was largely in line with estimates.",
            "Market reaction to the earnings announcement was mixed with varied responses.",
            "The company announced strategic initiatives to improve operational efficiency."
        ]

    # Run comparison
    results = compare_analyzers(texts)

    return results


if __name__ == "__main__":
    print("ğŸš€ Testing Optimized Sentiment Analyzer")
    print("=" * 45)

    # Test on real data
    test_results = test_on_real_data(n_samples=300)

    print(f"\nğŸ‰ FINAL RESULTS:")
    print(
        f"   Neutral bias reduction: {test_results['neutral_reduction']:.1f} percentage points")

    if test_results['neutral_reduction'] > 5:
        print("âœ… EXCELLENT: Significant bias reduction achieved!")
    elif test_results['neutral_reduction'] > 0:
        print("âœ… GOOD: Bias reduction achieved!")
    else:
        print("âš ï¸  NEEDS WORK: No bias reduction")

    print("\nâœ… Optimized sentiment analyzer testing completed!")
