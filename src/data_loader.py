# data_loader.py

import time
import requests
import pandas as pd
from transformers import AutoTokenizer, logging
from dotenv import load_dotenv
import os
import json
from datetime import datetime
from typing import Dict, List, Optional

# Load environment variables from .env file
load_dotenv()

# 1. Configuration
API_TOKEN = os.getenv("EODHD_API_TOKEN")
BASE_URL = "https://eodhd.com/api/news"

# Date range for articles
FROM_DATE = "2020-01-01"
TO_DATE = "2025-06-07"

# Target number of articles per tag
TARGET_PER_TAG = 2000

# Pagination batch size for each API call (500 is safe)
BATCH_SIZE = 500

# Sleep between requests to avoid hitting rate limits
SLEEP_SEC = 0.3

# Quality control parameters
MIN_CONTENT_WORDS = 50
MIN_TITLE_WORDS = 3
MAX_DUPLICATE_THRESHOLD = 0.85  # For content similarity detection

# Broad tags to query for diversified coverage
TAGS = [
    'balance sheet', 'capital employed', 'class action', 'company announcement',
    'consensus eps estimate', 'consensus estimate', 'credit rating',
    'discounted cash flow', 'dividend payments', 'earnings estimate',
    'earnings growth', 'earnings per share', 'earnings release', 'earnings report',
    'earnings results', 'earnings surprise', 'estimate revisions',
    'european regulatory news', 'financial results', 'fourth quarter',
    'free cash flow', 'future cash flows', 'growth rate', 'initial public offering',
    'insider ownership', 'insider transactions', 'institutional investors',
    'institutional ownership', 'intrinsic value', 'market research reports',
    'net income', 'operating income', 'present value', 'press releases',
    'price target', 'quarterly earnings', 'quarterly results', 'ratings',
    'research analysis and reports', 'return on equity', 'revenue estimates',
    'revenue growth', 'roce', 'roe', 'share price', 'shareholder rights',
    'shareholder', 'shares outstanding', 'split', 'strong buy', 'total revenue',
    'zacks investment research', 'zacks rank'
]

# Common patterns for low-quality or auto-generated content
LOW_QUALITY_PATTERNS = [
    "this article was generated by",
    "automatically generated",
    "computer generated",
    "ai generated",
    "algorithmic trading",
    "please see important disclosures",
    "this is not investment advice",
    "past performance does not guarantee",
    "disclaimer:",
    "for educational purposes only"
]

def validate_article_quality(article: Dict) -> bool:
    """
    Filter out low-quality articles based on multiple criteria.
    
    Args:
        article: Dictionary containing article data
        
    Returns:
        bool: True if article meets quality standards
    """
    content = article.get("content", "").strip()
    title = article.get("title", "").strip()
    
    # Check minimum length requirements
    if len(content.split()) < MIN_CONTENT_WORDS:
        return False
        
    if len(title.split()) < MIN_TITLE_WORDS:
        return False
    
    # Check for empty or placeholder content
    if not content or content.lower() in ["", "n/a", "no content", "content not available"]:
        return False
        
    if not title or title.lower() in ["", "n/a", "untitled"]:
        return False
    
    # Check for auto-generated or low-quality content patterns
    content_lower = content.lower()
    title_lower = title.lower()
    
    for pattern in LOW_QUALITY_PATTERNS:
        if pattern in content_lower or pattern in title_lower:
            return False
    
    # Check for repetitive content (simple heuristic)
    words = content.split()
    if len(set(words)) / len(words) < 0.3:  # Too many repeated words
        return False
    
    # Check for suspiciously short sentences (might be corrupted)
    sentences = content.split('.')
    avg_sentence_length = sum(len(s.split()) for s in sentences) / len(sentences) if sentences else 0
    if avg_sentence_length < 3:  # Very short sentences might indicate poor quality
        return False
    
    # Check if article has meaningful symbols (at least one potential ticker)
    symbols = article.get("symbols", [])
    if not symbols:
        # Look for potential ticker symbols in content (basic check)
        import re
        potential_tickers = re.findall(r'\b[A-Z]{2,5}\b', content + " " + title)
        if len(potential_tickers) < 1:
            return False
    
    return True

def detect_near_duplicates(article: Dict, existing_articles: List[Dict]) -> bool:
    """
    Simple duplicate detection based on title and content similarity.
    
    Args:
        article: New article to check
        existing_articles: List of already collected articles
        
    Returns:
        bool: True if article is likely a duplicate
    """
    new_title = article.get("title", "").lower().strip()
    new_content = article.get("content", "").lower().strip()
    
    # Simple similarity check (can be enhanced with more sophisticated methods)
    for existing in existing_articles[-100:]:  # Check against last 100 articles for efficiency
        existing_title = existing.get("title", "").lower().strip()
        existing_content = existing.get("content", "").lower().strip()
        
        # Title similarity
        if new_title and existing_title:
            title_similarity = len(set(new_title.split()) & set(existing_title.split())) / max(len(set(new_title.split())), 1)
            if title_similarity > 0.8:
                return True
        
        # Content similarity (first 200 words)
        new_words = set(new_content.split()[:200])
        existing_words = set(existing_content.split()[:200])
        if new_words and existing_words:
            content_similarity = len(new_words & existing_words) / len(new_words | existing_words)
            if content_similarity > MAX_DUPLICATE_THRESHOLD:
                return True
    
    return False

def fetch_page(tag: str, offset: int) -> Optional[List[Dict]]:
    """
    Fetch one page of articles for a given tag and offset.
    
    Args:
        tag: Search tag
        offset: Pagination offset
        
    Returns:
        List of articles or None if error
    """
    params = {
        "t": tag,
        "from": FROM_DATE,
        "to": TO_DATE,
        "limit": BATCH_SIZE,
        "offset": offset,
        "api_token": API_TOKEN,
        "fmt": "json"
    }
    
    try:
        response = requests.get(BASE_URL, params=params, timeout=30)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        print(f"Error fetching page at offset {offset} for tag '{tag}': {e}")
        return None
    except json.JSONDecodeError as e:
        print(f"Error parsing JSON response for tag '{tag}' at offset {offset}: {e}")
        return None

def label_by_max_prob(sent_dict: Dict) -> str:
    """
    Label sentiment by maximum probability.
    
    Args:
        sent_dict: Dictionary with sentiment probabilities
        
    Returns:
        Sentiment label string
    """
    if not sent_dict or not isinstance(sent_dict, dict):
        return "Neutral"
    
    neg = sent_dict.get("neg", 0.0)
    neu = sent_dict.get("neu", 0.0)
    pos = sent_dict.get("pos", 0.0)
    
    label, _ = max(
        [("Negative", neg), ("Neutral", neu), ("Positive", pos)],
        key=lambda x: x[1]
    )
    return label

def get_token_count(text: str, tokenizer) -> int:
    """
    Count tokens in text using the tokenizer.
    
    Args:
        text: Input text
        tokenizer: HuggingFace tokenizer
        
    Returns:
        Number of tokens
    """
    try:
        return len(tokenizer.tokenize(text))
    except Exception:
        # Fallback to word count if tokenization fails
        return len(text.split())

def save_checkpoint(articles: List[Dict], tag: str, checkpoint_dir: str = "data/checkpoints"):
    """
    Save checkpoint of collected articles.
    
    Args:
        articles: List of articles
        tag: Current tag being processed
        checkpoint_dir: Directory to save checkpoints
    """
    os.makedirs(checkpoint_dir, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    checkpoint_file = os.path.join(checkpoint_dir, f"checkpoint_{tag.replace(' ', '_')}_{timestamp}.json")
    
    with open(checkpoint_file, 'w', encoding='utf-8') as f:
        json.dump({
            'tag': tag,
            'count': len(articles),
            'timestamp': timestamp,
            'articles': articles[-100:]  # Save last 100 articles as sample
        }, f, indent=2, ensure_ascii=False)
    
    print(f"Checkpoint saved: {checkpoint_file}")

def main():
    """
    Main data collection function with enhanced quality control.
    """
    # Validate API token
    if not API_TOKEN:
        raise ValueError("EODHD_API_TOKEN not found in environment variables")
    
    # Initialize tokenizer for token count computation
    print("Loading tokenizer...")
    logging.set_verbosity_error()
    try:
        tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
    except Exception as e:
        print(f"Warning: Could not load tokenizer: {e}")
        tokenizer = None
    
    # Initialize collected articles list
    all_articles = []
    quality_stats = {
        'total_fetched': 0,
        'quality_filtered': 0,
        'duplicate_filtered': 0,
        'final_count': 0
    }
    
    print(f"Starting data collection for {len(TAGS)} tags...")
    print(f"Target per tag: {TARGET_PER_TAG}")
    print(f"Date range: {FROM_DATE} to {TO_DATE}")
    
    # Main collection loop
    for tag_idx, tag in enumerate(TAGS, 1):
        print(f"\n[{tag_idx}/{len(TAGS)}] Processing tag: '{tag}'")
        offset = 0
        tag_collected = {}  # dedupe per tag using link as key
        tag_quality_filtered = 0
        tag_duplicate_filtered = 0
        
        while len(tag_collected) < TARGET_PER_TAG:
            data = fetch_page(tag, offset)
            
            if data is None:
                print(f"Retrying after error...")
                time.sleep(5)
                continue
                
            if not data:
                print(f"No more articles found for tag: '{tag}'")
                break
            
            new_count = 0
            for art in data:
                quality_stats['total_fetched'] += 1
                
                link = art.get("link")
                if not link:
                    continue
                
                if link in tag_collected:
                    continue  # Already have this article
                
                # Quality validation
                if not validate_article_quality(art):
                    quality_stats['quality_filtered'] += 1
                    tag_quality_filtered += 1
                    continue
                
                # Duplicate detection (against articles from this tag)
                if detect_near_duplicates(art, list(tag_collected.values())):
                    quality_stats['duplicate_filtered'] += 1
                    tag_duplicate_filtered += 1
                    continue
                
                # Add valid article
                tag_collected[link] = {
                    "date": art.get("date"),
                    "title": art.get("title", ""),
                    "content": art.get("content", ""),
                    "symbols": art.get("symbols", []),
                    "tags": art.get("tags", []),
                    "sentiment": art.get("sentiment", {}),
                    "tag_source": tag,
                    "link": link
                }
                new_count += 1
            
            print(f"  Offset {offset}: fetched {len(data)}, +{new_count} valid, "
                  f"total for tag = {len(tag_collected)}")
            
            if len(tag_collected) >= TARGET_PER_TAG:
                break
                
            offset += BATCH_SIZE
            time.sleep(SLEEP_SEC)
        
        # Add tag articles to global list
        tag_articles = list(tag_collected.values())
        all_articles.extend(tag_articles)
        
        print(f"‚úÖ Tag '{tag}' completed:")
        print(f"   Collected: {len(tag_articles)} articles")
        print(f"   Quality filtered: {tag_quality_filtered}")
        print(f"   Duplicate filtered: {tag_duplicate_filtered}")
        
        # Save checkpoint every 10 tags
        if tag_idx % 10 == 0:
            save_checkpoint(all_articles, f"tags_1_to_{tag_idx}")
    
    print(f"\nüéØ Collection Summary:")
    print(f"Total articles fetched: {quality_stats['total_fetched']}")
    print(f"Quality filtered out: {quality_stats['quality_filtered']}")
    print(f"Duplicates filtered out: {quality_stats['duplicate_filtered']}")
    print(f"Final collection size: {len(all_articles)}")
    
    if not all_articles:
        print("‚ùå No articles collected. Check your API token and network connection.")
        return
    
    # Process articles for final dataset
    print("\nProcessing articles for final dataset...")
    
    for art in all_articles:
        # Add sentiment label
        art["sentiment_label"] = label_by_max_prob(art["sentiment"])
        
        # Add token count
        if tokenizer:
            full_text = art["title"] + "\n\n" + art["content"]
            art["token_count"] = get_token_count(full_text, tokenizer)
        else:
            # Fallback to word count
            full_text = art["title"] + "\n\n" + art["content"]
            art["token_count"] = len(full_text.split())
    
    # Build final DataFrame
    df = pd.DataFrame(all_articles)
    df = df[[
        "date", "title", "content", "symbols", "tags", "tag_source",
        "sentiment_label", "sentiment", "token_count", "link"
    ]]
    
    # Ensure data directory exists
    os.makedirs("data", exist_ok=True)
    
    # Save to Parquet
    output_file = "data/financial_news_2020_2025_100k.parquet"
    df.to_parquet(output_file, index=False)
    print(f"‚úÖ Dataset saved to {output_file}")
    
    # Save metadata
    metadata = {
        "collection_date": datetime.now().isoformat(),
        "total_articles": len(df),
        "date_range": f"{FROM_DATE} to {TO_DATE}",
        "tags_used": TAGS,
        "quality_stats": quality_stats,
        "columns": list(df.columns)
    }
    
    metadata_file = "data/collection_metadata.json"
    with open(metadata_file, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    print(f"‚úÖ Metadata saved to {metadata_file}")
    
    # Display sample statistics
    print(f"\nüìä Dataset Statistics:")
    print(f"Articles by sentiment:")
    print(df['sentiment_label'].value_counts())
    print(f"\nArticles by tag source (top 10):")
    print(df['tag_source'].value_counts().head(10))
    print(f"\nToken count statistics:")
    print(df['token_count'].describe())

if __name__ == "__main__":
    main()