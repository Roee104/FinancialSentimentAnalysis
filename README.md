# ğŸ“˜ Full Project Story â€“ *FinancialSentimentAnalysis*

---

## ğŸ“š Table of Contents

1. [Overview](#overview)
2. [Initial Vision & Proposal](#1-initial-vision--proposal)
3. [Data Collection â€” Phase I](#2-data-collection--phase-i)
4. [Building a Gold Standard Dataset](#3-building-a-gold-standard-dataset)
5. [Interim Stage â€” Analysis, EDA & Failures](#4-interim-stage--analysis-eda--failures)
6. [Pipeline Implementation](#5-pipeline-implementation)
7. [Pivot: From Multi-Level to Overall Sentiment](#6-pivot-from-multi-level-to-overall-sentiment)
8. [Modeling & Results](#7-modeling--results)
9. [Technical Challenges](#8-technical-challenges)
10. [Final Report Stage](#9-final-report-stage)
11. [Final Deliverable Summary](#final-deliverable-summary)
12. [Reflection](#reflection)
13. [How to Run the Project](#how-to-run-the-project)
    - [Installation & Setup](#1-installation--setup)
    - [Environment Configuration](#2-environment-configuration)
    - [Running the Pipeline](#3-running-the-pipeline)
    - [Outputs](#4-outputs)
    - [Evaluating Performance](#5-evaluating-performance)
    - [Common Issues & Fixes](#6-common-issues--fixes)
    - [File Structure Overview](#7-file-structure-overview)
    - [Fine-Tuning a LoRA Adapter Model](#8-optional-fine-tuning-a-lora-adapter-model)
14. [Dataset Examples](#dataset-examples)

---

## ğŸ”– Overview

This project set out to solve a challenging but practically important task: to analyze **financial news articles** and automatically determine their **sentiment** â€” overall and, ideally, for specific **tickers** and **sectors**. What began as an ambitious multi-component system eventually evolved â€” through careful iteration, failed experiments, and problem-solving â€” into a working, high-performing solution for **overall sentiment classification**, built around a fine-tuned FinBERT model using LoRA.

What follows is a chronological, detailed account of everything we did â€” including what worked, what failed, and why.

---

## 1. ğŸ§  Initial Vision & Proposal

We started with a **multi-objective plan**:

- Accept raw financial articles (headline + body),
- Automatically extract all **tickers** mentioned in the article,
- Link each ticker to its **sector** using external CSVs,
- Predict sentiment at:
  - Ticker level,
  - Sector level,
  - Article (overall) level,
- Provide a **confidence score** and **rationale** (explanation) for each prediction.

### ğŸ§© Early Design Decisions

- Planned to use **FinSPID** and **Yahoo Finance** for data,
- Intended to use **Snorkel** for distant supervision (rejected later),
- Chose **FinBERT-NER** for ticker recognition, and **DeBERTa** and **FinBERT** for classification,
- Designed a modular pipeline structure with:
  - `core/` for modeling,
  - `data/` for loading and validation,
  - `scripts/` for orchestration.

---

## 2. ğŸ“¦ Data Collection â€” Phase I

We soon faced major problems with data.

### âŒ FINSPID Rejected

- Too narrow, too few samples, no real article bodies.

### âœ… Switched to EODHD API

- Collected 100k financial articles into `financial_news_2020_2025_100k.parquet`,
- Each entry had headline, body, datetime â€” but all were labeled `"neutral"` by default.

> ğŸ’¡ **Key Insight:** We couldnâ€™t use this dataset as-is for training or even filtering by sentiment.

---

## 3. ğŸ§¹ Building a Gold Standard Dataset

To enable training and evaluation, we generated our own **gold-standard labeled dataset**:

- Used GPT-4 to annotate ~3000 articles with:
  - `overall_sentiment`: Positive / Neutral / Negative,
  - `confidence`,
  - `rationale` (optional),
  - `ticker_sentiments` (eventually dropped).
- Saved in `final_gold_standard_3000.jsonl`.

### ğŸ›‘ Problems Encountered:

- Skewed class distribution: 65% Positive, 30% Neutral, 5% Negative,
- Noisy labels: some `"mixed"` or empty,
- Multiple incompatible label field names: `label`, `true_overall`, `overall_sentiment`,
- Evaluation metrics became unreliable unless cleaned manually.

---

## 4. ğŸ§ª Interim Stage â€” Analysis, EDA & Failures

### âœ… EDA Notebooks

- We created EDA visualizations to explore class distributions and spot label noise.
- Found that many â€œNeutralâ€ samples had mixed cues, and Positive/Negative were sometimes misclassified.

### âŒ Failed Attempts

- Initial pipeline with LoRA-adapted FinBERT failed completely:
  - Predicted only `Neutral` on every sample,
  - Loss function possibly broken (no gradient),
  - Data pipeline passed wrong label mappings.

### ğŸ©¹ Fixes Tried:

- Dropped `"mixed"` samples entirely,
- Balanced classes to reach 3000 samples per class (created final_dataset.jsonl),
- Refactored data loader to normalize field names,
- Verified label distribution per epoch using checkpoints.

---

## 5. âš™ï¸ Pipeline Implementation

We built a flexible pipeline composed of:

- `data.loader.py`: to load and validate JSONL / Parquet data,
- `core.sentiment.py`: logic for FinBERT, DeBERTa, LoRA support,
- `scripts/run_pipeline.py`: main orchestration entry point,
- `scripts/run_finetuned_pipelines.py`: batch inference with checkpoints.

Other modules:

- `core.text_processor.py`: cleaned headlines and bodies,
- `core.pretrained_financial_ner.py`: attempted FinBERT-NER integration (sidelined),
- `core.aggregator.py`: for combining predictions at sector/ticker levels (later unused),
- `config/settings.py`: held config paths and logging.

---

## 6. ğŸ” Pivot: From Multi-Level to Overall Sentiment

Realizing the full pipeline was too ambitious, we narrowed the scope:

- âŒ Dropped per-ticker sentiment,
- âŒ Dropped sector-level sentiment,
- âŒ Dropped rationale and span mapping,
- âœ… Focused 100% on predicting **one sentiment label per article** with confidence.

This decision was crucial to deliver a functioning and testable system by the final stage.

---

## 7. ğŸ§  Modeling & Results

### âœ… Models Used:

- **VADER** (lexicon baseline),
- **FinBERT** (pretrained),
- **FinBERT-LoRA** (fine-tuned on gold dataset),
- **DeBERTa-LoRA** (alternative transformer model).

### ğŸ“Š Final Results:

| Model                   | Macro-F1  | Accuracy  |
|-------------------------|-----------|-----------|
| VADER                   | ~42%      | 53.1%     |
| FinBERT (baseline)      | ~64%      | 64.5%     |
| DeBERTa-LoRA            | 71.8%     | 72.2%     |
| **FinBERT-LoRA (ours)** | **73.3%** | **73.6%** |

---

## 8. ğŸ§° Technical Challenges

We hit several critical issues:

- Inconsistent labeling caused model collapse,
- Confusion matrix showed **all predictions as Neutral** until we rebalanced data,
- Confidence scores were meaningless (always 0.5) before final fixes.

We documented and fixed each issue progressively.

---

## 9. ğŸ§¾ Final Report Stage

We prepared:

- **README.md** with detailed install & run instructions,
- **Performance table** for baselines vs. fine-tuned models,
- **Graphical abstract** (3-panel summary of problem â†’ method â†’ result),
- **Presentation slides** outlining:
  - Motivation,
  - Pipeline,
  - Dataset issues,
  - Evaluation outcomes,
  - Realistic scope discussion.

---

## ğŸ”š Final Deliverable Summary

We delivered:

- A working, accurate **overall sentiment predictor** for financial news articles.
- Trained models that clearly outperform baselines.
- A structured, reproducible pipeline with proper configuration and evaluation.
- A complete final submission with transparency around what worked and what didnâ€™t.

---

## ğŸ“ Reflection

> â€œWe set out to solve more than we could â€” and learned more than we expected.â€

This project taught us:

- How to **balance ambition with feasibility**,
- How to debug modern NLP pipelines,
- The importance of **data quality** over model complexity,
- How to accept trade-offs and communicate them clearly.

---

# ğŸ› ï¸ How to Run the Project

This guide outlines how to set up and run the **FinancialSentimentAnalysis** project from start to finish â€” including installation, data preparation, and executing the sentiment analysis pipeline.

---

## 1. ğŸ“¦ Installation & Setup

### âœ… Prerequisites:

- Python 3.9+
- `pip`
- Optional: Virtual environment (`venv` or `conda`)

### ğŸ“ Step 1: Clone / Extract the Project

If zipped, unzip it:

```bash
unzip FinancialSentimentAnalysis-main.zip
cd FinancialSentimentAnalysis-main
````

Or if from Git:

```bash
git clone <repo-url>
cd FinancialSentimentAnalysis
```

### ğŸ“¥ Step 2: Create & Activate Virtual Environment

```bash
python -m venv venv
source venv/bin/activate        # On Windows: venv\Scripts\activate
```

### ğŸ“š Step 3: Install Dependencies

```bash
pip install -e .
```

Or:

```bash
pip install -r requirements.txt
```

---

## 2. âš™ï¸ Environment Configuration

### ğŸ—‚ï¸ Step 4: Add `.env` File

Create a file named `.env` in the root folder with the following content (if applicable):

```env
EODHD_API_TOKEN=your_token_here
```

> âš ï¸ If youâ€™re not fetching live data, this can be left empty.

---

## 3. ğŸ“Š Running the Pipeline

### â–¶ï¸ Option 1: Run on Preloaded Gold Dataset

This is the most stable, plug-and-play execution mode.

```bash
python scripts/run_pipeline.py --input data/final_gold_standard_9000.jsonl --model finbert_lora
```

Valid model names:

* `vader`
* `finbert`
* `finbert_lora`
* `deberta_lora`

You can also run:

```bash
python scripts/run_finetuned_pipelines.py
```

This will evaluate all fine-tuned models on the gold dataset and output predictions.

---

## 4. ğŸ“ˆ Outputs

Results will be saved to:

* `outputs/predictions/` â€“ predictions with sentiment + confidence
* `outputs/evaluation/` â€“ metrics and confusion matrix
* `outputs/plots/` â€“ visualizations (optional)

Example prediction format:

```json
{
  "headline": "Tech Stocks Rally as Fed Holds Rates",
  "body": "...",
  "overall_sentiment": "Positive",
  "confidence": 0.91
}
```

---

## 5. ğŸ§ª Evaluating Performance

To evaluate the results:

```bash
python scripts/run_pipeline.py --evaluate
```

Make sure the gold-standard file includes true labels in the field: `overall_sentiment`.

---

## 6. ğŸ› Common Issues & Fixes

| Issue                                           | Fix                                                                   |
| ----------------------------------------------- | --------------------------------------------------------------------- |
| `ModuleNotFoundError: No module named 'config'` | Ensure you ran `pip install -e .` in the root directory               |
| Predictions all `Neutral`                       | Check that gold file has no `"mixed"` labels, and dataset is balanced |
| `.env` not found                                | Create manually with at least one variable (can be empty)             |
| Label field missing                             | Use `true_overall` or standardize to `overall_sentiment`              |

---

## 7. ğŸ“‚ File Structure Overview

```bash
.
â”œâ”€â”€ config/               # Configuration files
â”œâ”€â”€ core/                 # Main logic (NER, sentiment, etc.)
â”œâ”€â”€ data/                 # Datasets and loaders
â”œâ”€â”€ scripts/              # Pipeline entry points
â”œâ”€â”€ outputs/              # Evaluation + prediction results
â”œâ”€â”€ notebooks/            # EDA and experimentation
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â””â”€â”€ README.md
```

---

## ğŸ§  8. (Optional) Fine-Tuning a LoRA Adapter Model

If you want to fine-tune your own adapter on top of a pretrained model (e.g., FinBERT), use the `scripts.run_experiments` module.

Here is the exact command we used in this project to fine-tune FinBERT with LoRA:

```bash
python3 -m scripts.run_experiments \
  --model finbert \
  --lora \
  --epochs 50 \
  --lr 2e-4 \
  --rank 8 \
  --alpha 32 \
  --train data/train_set.jsonl \
  --val data/val_set.jsonl \
  --lr_scheduler reduce \
  --metric_for_best_model eval_f1
```

> ğŸ’¡ Note: Label smoothing was used in training and set to `0.05`.

### ğŸ“Œ Explanation of Parameters:

| Argument                          | Meaning                                 |
| --------------------------------- | --------------------------------------- |
| `--model finbert`                 | Use pretrained FinBERT                  |
| `--lora`                          | Enable LoRA fine-tuning                 |
| `--epochs 50`                     | Train for 50 epochs                     |
| `--lr 2e-4`                       | Learning rate                           |
| `--rank 8`, `--alpha 32`          | LoRA-specific hyperparameters           |
| `--train` / `--val`               | Paths to training/validation files      |
| `--lr_scheduler reduce`           | Use learning rate reduction on plateau  |
| `--metric_for_best_model eval_f1` | Save the model that performs best on F1 |

### ğŸ—‚ Output:

* The fine-tuned LoRA adapter will be saved under:

```
models/finbert_lora/
```

* You can then evaluate this model using:

```bash
python scripts/run_pipeline.py --model finbert_lora --input data/final_gold_standard_9000.jsonl
```

---

## ğŸ“¸ Dataset Examples

Below are example previews of the two key datasets used in this project.

---

### ğŸ“„ `financial_news_2020_2025_100k.parquet`

This is the raw dataset collected via the EODHD API. 

![Raw Financial Dataset](images/financial_news_2020_2025_100k_preview.png)

---

### ğŸ·ï¸ `final_dataset.jsonl`

This is the cleaned and balanced dataset manually labeled with GPT-4. 

![Gold Standard Dataset](images/final_dataset_preview.png)

---


